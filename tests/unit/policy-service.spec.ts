/**
 * PolicyService Unit Tests
 * Tests for src/server/policy/services/policy-service.ts
 * 
 * Generated by BMAD testarch-automate workflow
 * Priority: P0 (Critical) - PolicyService is the single entry point for all policy evaluations
 * Coverage Target: critical-paths
 */

import { test, expect, describe, beforeEach, vi } from 'vitest';
import {
  PolicyService,
  getPolicyService,
  resetPolicyService,
} from '../../src/server/policy/services/policy-service';
import { PolicyEngine } from '../../src/server/policy/engine';
import {
  PolicyConfig,
  PredictionInput,
  RunContext,
  PolicyEvaluationResult,
  PolicyEvaluationResponse,
  PolicyErrorResponse,
  PolicyError,
} from '../../src/server/policy/types';

// Test fixtures
const createMockPrediction = (overrides?: Partial<PredictionInput>): PredictionInput => ({
  id: 'pred-123',
  matchId: 'match-456',
  runId: 'run-789',
  userId: 'user-001',
  confidence: 0.75,
  edge: 0.08,
  driftScore: 0.05,
  winnerPrediction: 'TeamA',
  scorePrediction: '105-98',
  overUnderPrediction: 205.5,
  modelVersion: 'v2.1.0',
  ...overrides,
});

const createMockContext = (overrides?: Partial<RunContext>): RunContext => ({
  runId: 'run-789',
  traceId: 'trace-abc-123',
  dailyLoss: 150,
  consecutiveLosses: 3,
  currentBankroll: 1000,
  executedAt: new Date('2024-01-15T10:30:00Z'),
  ...overrides,
});

const createMockEngine = () => {
  const mockEvaluate = vi.fn().mockImplementation(
    async (prediction: PredictionInput, context: RunContext): Promise<PolicyEvaluationResult> => {
      // Simulate engine validation - these throw errors in real implementation
      if (!prediction.id) {
        throw new PolicyError('Prediction ID is required', 'INVALID_INPUT');
      }
      if (prediction.confidence === undefined || prediction.confidence === null) {
        throw new PolicyError('Prediction confidence is required', 'INVALID_INPUT');
      }
      if (prediction.confidence < 0 || prediction.confidence > 1) {
        throw new PolicyError(
          `Invalid confidence value: ${prediction.confidence}. Must be between 0 and 1.`,
          'INVALID_INPUT'
        );
      }
      
      // Simulate engine evaluation
      const passed = prediction.confidence >= 0.65;
      return {
        decisionId: 'decision-001',
        predictionId: prediction.id,
        status: passed ? 'PICK' : 'NO_BET',
        rationale: passed ? 'PICK: All gates passed' : 'NO-BET: Confidence below threshold',
        confidenceGate: passed,
        edgeGate: true,
        driftGate: true,
        hardStopGate: true,
        hardStopReason: null,
        recommendedAction: passed ? 'Proceed with bet' : 'Review metrics',
        traceId: context.traceId,
        executedAt: context.executedAt,
        gateOutcomes: {
          confidence: { passed, score: prediction.confidence, threshold: 0.65 },
          edge: { passed: true, score: prediction.edge || 0, threshold: 0.05 },
          drift: { passed: true, score: prediction.driftScore || 0, threshold: 0.15 },
          hardStop: { passed: true },
        },
      };
    }
  );

  const mockGetConfig = vi.fn().mockReturnValue({
    confidence: { minThreshold: 0.65 },
    edge: { minThreshold: 0.05 },
    drift: { maxDriftScore: 0.15 },
    hardStops: { dailyLossLimit: 1000, consecutiveLosses: 5, bankrollPercent: 0.10 },
  });

  return {
    evaluate: mockEvaluate,
    getConfig: mockGetConfig,
  } as unknown as PolicyEngine;
};

describe('PolicyService @unit @policy-service @p0', () => {
  let policyService: PolicyService;
  let mockEngine: PolicyEngine;

  beforeEach(() => {
    // Reset singleton before each test
    resetPolicyService();
    mockEngine = createMockEngine();
    policyService = new PolicyService({ policyEngine: mockEngine });
  });

  describe('evaluatePrediction - Core Evaluation', () => {
    test('[P0] should evaluate prediction and return result', async () => {
      // Given valid prediction and context
      const prediction = createMockPrediction({ confidence: 0.80 });
      const context = createMockContext();

      // When evaluating
      const result = await policyService.evaluatePrediction(prediction, context);

      // Then should return evaluation result
      expect(result).toBeDefined();
      expect(result.predictionId).toBe(prediction.id);
      expect(result.traceId).toBe(context.traceId);
    });

    test('[P0] should throw PolicyError for missing prediction ID', async () => {
      // Given prediction without ID
      const prediction = createMockPrediction({ id: '' });
      const context = createMockContext();

      // When/Then should throw
      await expect(policyService.evaluatePrediction(prediction, context)).rejects.toThrow(
        'Prediction ID is required'
      );
    });

    test('[P0] should throw PolicyError for invalid prediction', async () => {
      // Given prediction with invalid confidence
      const prediction = createMockPrediction({ confidence: 1.5 }); // > 1
      const context = createMockContext();

      // When/Then should throw
      await expect(policyService.evaluatePrediction(prediction, context)).rejects.toThrow(
        'Invalid confidence value'
      );
    });

    test('[P1] should handle null confidence', async () => {
      // Given prediction with null confidence
      const prediction = { ...createMockPrediction(), confidence: null as any };
      const context = createMockContext();

      // When/Then should throw
      await expect(policyService.evaluatePrediction(prediction, context)).rejects.toThrow(
        'Prediction confidence is required'
      );
    });

    test('[P1] should pass context to engine', async () => {
      // Given prediction and context with specific values
      const prediction = createMockPrediction();
      const context = createMockContext({
        dailyLoss: 500,
        consecutiveLosses: 4,
        traceId: 'test-trace-id',
      });

      // When evaluating
      await policyService.evaluatePrediction(prediction, context);

      // Then engine should receive context
      const engine = mockEngine as any;
      expect(engine.evaluate).toHaveBeenCalledWith(prediction, context);
    });
  });

  describe('getConfig - Configuration Access', () => {
    test('[P0] should return current policy configuration', () => {
      // When getting config
      const config = policyService.getConfig();

      // Then should return valid config
      expect(config).toBeDefined();
      expect(config.confidence).toBeDefined();
      expect(config.edge).toBeDefined();
      expect(config.drift).toBeDefined();
      expect(config.hardStops).toBeDefined();
    });

    test('[P1] should return default thresholds', () => {
      // When getting config
      const config = policyService.getConfig();

      // Then should have expected default values
      expect(config.confidence.minThreshold).toBe(0.65);
      expect(config.edge.minThreshold).toBe(0.05);
      expect(config.drift.maxDriftScore).toBe(0.15);
    });
  });

  describe('formatApiResponse - Response Formatting', () => {
    test('[P0] should format successful result for API', () => {
      // Given evaluation result
      const result: PolicyEvaluationResult = {
        decisionId: 'decision-001',
        predictionId: 'pred-123',
        status: 'PICK',
        rationale: 'PICK: All gates passed',
        confidenceGate: true,
        edgeGate: true,
        driftGate: true,
        hardStopGate: true,
        hardStopReason: null,
        recommendedAction: 'Proceed with bet',
        traceId: 'trace-abc',
        executedAt: new Date('2024-01-15T10:30:00Z'),
        gateOutcomes: {
          confidence: { passed: true, score: 0.75, threshold: 0.65 },
          edge: { passed: true, score: 0.08, threshold: 0.05 },
          drift: { passed: true, score: 0.05, threshold: 0.15 },
          hardStop: { passed: true },
        },
      };

      // When formatting
      const response = policyService.formatApiResponse(result);

      // Then should have expected structure
      expect(response.data).toBeDefined();
      expect(response.data.decisionId).toBe(result.decisionId);
      expect(response.data.status).toBe(result.status);
      expect(response.data.rationale).toBe(result.rationale);
      expect(response.meta).toBeDefined();
      expect(response.meta.traceId).toBe(result.traceId);
      expect(response.meta.timestamp).toBe(result.executedAt.toISOString());
    });

    test('[P1] should format hard-stop response correctly', () => {
      // Given hard-stop result
      const result: PolicyEvaluationResult = {
        decisionId: 'decision-002',
        predictionId: 'pred-456',
        status: 'HARD_STOP',
        rationale: 'HARD-STOP: Daily loss limit exceeded',
        confidenceGate: true,
        edgeGate: true,
        driftGate: true,
        hardStopGate: false,
        hardStopReason: 'Daily loss limit of $1000 exceeded',
        recommendedAction: 'Stop betting for today',
        traceId: 'trace-def',
        executedAt: new Date('2024-01-15T11:00:00Z'),
        gateOutcomes: {
          confidence: { passed: true, score: 0, threshold: 0 },
          edge: { passed: true, score: 0, threshold: 0 },
          drift: { passed: true, score: 0, threshold: 0 },
          hardStop: { passed: false },
        },
      };

      // When formatting
      const response = policyService.formatApiResponse(result);

      // Then should preserve all fields
      expect(response.data.status).toBe('HARD_STOP');
      expect(response.data.gateOutcomes.hardStop.passed).toBe(false);
    });

    test('[P2] should format NO_BET response correctly', () => {
      // Given no-bet result
      const result: PolicyEvaluationResult = {
        decisionId: 'decision-003',
        predictionId: 'pred-789',
        status: 'NO_BET',
        rationale: 'NO-BET: Confidence below threshold',
        confidenceGate: false,
        edgeGate: true,
        driftGate: true,
        hardStopGate: true,
        hardStopReason: null,
        recommendedAction: 'Review prediction metrics',
        traceId: 'trace-ghi',
        executedAt: new Date('2024-01-15T12:00:00Z'),
        gateOutcomes: {
          confidence: { passed: false, score: 0.50, threshold: 0.65 },
          edge: { passed: true, score: 0.08, threshold: 0.05 },
          drift: { passed: true, score: 0.05, threshold: 0.15 },
          hardStop: { passed: true },
        },
      };

      // When formatting
      const response = policyService.formatApiResponse(result);

      // Then should have NO_BET status
      expect(response.data.status).toBe('NO_BET');
      expect(response.data.gateOutcomes.confidence.passed).toBe(false);
    });
  });

  describe('formatErrorResponse - Error Formatting', () => {
    test('[P0] should format PolicyError correctly', () => {
      // Given a PolicyError
      const error = new PolicyError('Invalid input', 'INVALID_INPUT', { field: 'predictionId' });
      const traceId = 'error-trace-123';

      // When formatting
      const response = policyService.formatErrorResponse(error, traceId);

      // Then should have error structure
      expect(response.error).toBeDefined();
      expect(response.error.code).toBe('INVALID_INPUT');
      expect(response.error.message).toBe('Invalid input');
      expect(response.error.details).toEqual({ field: 'predictionId' });
      expect(response.meta.traceId).toBe(traceId);
      expect(response.meta.timestamp).toBeDefined();
    });

    test('[P1] should format generic Error correctly', () => {
      // Given a generic Error
      const error = new Error('Unexpected failure');
      const traceId = 'error-trace-456';

      // When formatting
      const response = policyService.formatErrorResponse(error, traceId);

      // Then should have fallback code
      expect(response.error.code).toBe('POLICY_EVALUATION_ERROR');
      expect(response.error.message).toBe('Unexpected failure');
      expect(response.meta.traceId).toBe(traceId);
    });

    test('[P2] should include timestamp in error response', () => {
      // Given an error
      const error = new Error('Test error');
      const traceId = 'test-trace';

      // When formatting
      const response = policyService.formatErrorResponse(error, traceId);

      // Then should have ISO timestamp
      expect(response.meta.timestamp).toMatch(/^\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}/);
    });
  });

  describe('Singleton Pattern', () => {
    test('[P1] should return same instance', () => {
      // When getting service twice
      const service1 = getPolicyService();
      const service2 = getPolicyService();

      // Then should be same instance
      expect(service1).toBe(service2);
    });

    test('[P1] should reset instance', () => {
      // Given singleton instance
      const service1 = getPolicyService();

      // When resetting
      resetPolicyService();
      const service2 = getPolicyService();

      // Then should be new instance
      expect(service1).not.toBe(service2);
    });
  });

  describe('Edge Cases', () => {
    test('[P2] should handle missing optional fields in prediction', async () => {
      // Given prediction with minimal required fields
      const prediction: PredictionInput = {
        id: 'pred-minimal',
        matchId: 'match-001',
        runId: 'run-001',
        userId: 'user-001',
        confidence: 0.70,
        modelVersion: 'v1.0.0',
      };
      const context = createMockContext();

      // When evaluating (should not throw)
      const result = await policyService.evaluatePrediction(prediction, context);

      // Then should return result
      expect(result).toBeDefined();
    });

    test('[P2] should handle edge value confidence (0 and 1)', async () => {
      // Given predictions with boundary values
      const context = createMockContext();
      
      // Test confidence = 0
      const result0 = await policyService.evaluatePrediction(
        createMockPrediction({ confidence: 0 }),
        context
      );
      expect(result0).toBeDefined();

      // Test confidence = 1
      const result1 = await policyService.evaluatePrediction(
        createMockPrediction({ confidence: 1 }),
        context
      );
      expect(result1).toBeDefined();
    });

    test('[P3] should preserve traceId through service', async () => {
      // Given context with specific traceId
      const prediction = createMockPrediction();
      const context = createMockContext({ traceId: 'unique-trace-123' });

      // When evaluating
      const result = await policyService.evaluatePrediction(prediction, context);

      // Then should preserve traceId
      expect(result.traceId).toBe('unique-trace-123');
    });
  });
});

describe('PolicyService Integration Patterns @integration @policy-service', () => {
  test('[P0] should work with default engine', async () => {
    // Given fresh service with default engine
    const service = new PolicyService();
    const prediction = createMockPrediction();
    const context = createMockContext();

    // When evaluating
    const result = await service.evaluatePrediction(prediction, context);

    // Then should work
    expect(result).toBeDefined();
    expect(result.predictionId).toBe(prediction.id);
  });

  test('[P1] should use custom engine configuration', () => {
    // Given custom configuration
    const customConfig: Partial<PolicyConfig> = {
      confidence: { minThreshold: 0.80 }, // Higher threshold
      edge: { minThreshold: 0.10 },
    };
    const engine = PolicyEngine.create(customConfig);
    const service = new PolicyService({ policyEngine: engine });

    // When getting config
    const config = service.getConfig();

    // Then should use custom values
    expect(config.confidence.minThreshold).toBe(0.80);
    expect(config.edge.minThreshold).toBe(0.10);
  });
});
