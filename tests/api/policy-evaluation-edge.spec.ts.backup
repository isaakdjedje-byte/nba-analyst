/**
 * Policy Evaluation Edge Cases API Tests
 * Tests for policy gate edge cases and boundary conditions
 *
 * Generated by BMAD testarch-automate workflow
 * Priority: P1 (High)
 */

import { test, expect } from '../support/merged-fixtures';
import { createDecision, createHardStopDecision, createNoBetDecision } from '../support/factories';

test.describe('Policy Evaluation Edge Cases - P1 @p1 @api @policy @edge-cases', () => {
  test('[P0] should handle confidence at exact threshold (0.5)', async ({ request }) => {
    // Given model outputs at confidence threshold
    const modelOutputs = {
      winner: 'Team A',
      confidence: 0.5, // Exactly at threshold
      edge: 0.05,
      drift: 0.05,
    };

    // When evaluating policy
    const response = await request.post('/api/policy/evaluate', {
      data: { modelOutputs },
    });

    // Then confidence gate should pass (>= 0.5)
    expect(response.status()).toBe(200);
    const result = await response.json();
    expect(result.gates.confidenceGate).toBe('passed');
    expect(result.status).toBe('Pick');
  });

  test('[P0] should handle confidence just below threshold (0.49)', async ({ request }) => {
    // Given model outputs just below confidence threshold
    const modelOutputs = {
      winner: 'Team A',
      confidence: 0.49, // Just below threshold
      edge: 0.05,
      drift: 0.05,
    };

    // When evaluating policy
    const response = await request.post('/api/policy/evaluate', {
      data: { modelOutputs },
    });

    // Then confidence gate should fail
    expect(response.status()).toBe(200);
    const result = await response.json();
    expect(result.gates.confidenceGate).toBe('failed');
    expect(result.status).toBe('No-Bet');
  });

  test('[P0] should handle edge at exact threshold (0.02)', async ({ request }) => {
    // Given model outputs at edge threshold
    const modelOutputs = {
      winner: 'Team A',
      confidence: 0.85,
      edge: 0.02, // Exactly at threshold
      drift: 0.05,
    };

    // When evaluating policy
    const response = await request.post('/api/policy/evaluate', {
      data: { modelOutputs },
    });

    // Then edge gate should pass (>= 0.02)
    expect(response.status()).toBe(200);
    const result = await response.json();
    expect(result.gates.edgeGate).toBe('passed');
  });

  test('[P0] should handle edge just below threshold (0.019)', async ({ request }) => {
    // Given model outputs just below edge threshold
    const modelOutputs = {
      winner: 'Team A',
      confidence: 0.85,
      edge: 0.019, // Just below threshold
      drift: 0.05,
    };

    // When evaluating policy
    const response = await request.post('/api/policy/evaluate', {
      data: { modelOutputs },
    });

    // Then edge gate should fail
    expect(response.status()).toBe(200);
    const result = await response.json();
    expect(result.gates.edgeGate).toBe('failed');
    expect(result.status).toBe('No-Bet');
  });

  test('[P0] should handle drift at exact threshold (0.1)', async ({ request }) => {
    // Given model outputs at drift threshold
    const modelOutputs = {
      winner: 'Team A',
      confidence: 0.85,
      edge: 0.05,
      drift: 0.1, // Exactly at threshold
    };

    // When evaluating policy
    const response = await request.post('/api/policy/evaluate', {
      data: { modelOutputs },
    });

    // Then drift gate should fail (drift >= 0.1 fails)
    expect(response.status()).toBe(200);
    const result = await response.json();
    expect(result.gates.driftGate).toBe('failed');
  });

  test('[P0] should handle drift just below threshold (0.099)', async ({ request }) => {
    // Given model outputs just below drift threshold
    const modelOutputs = {
      winner: 'Team A',
      confidence: 0.85,
      edge: 0.05,
      drift: 0.099, // Just below threshold
    };

    // When evaluating policy
    const response = await request.post('/api/policy/evaluate', {
      data: { modelOutputs },
    });

    // Then drift gate should pass (drift < 0.1 passes)
    expect(response.status()).toBe(200);
    const result = await response.json();
    expect(result.gates.driftGate).toBe('passed');
  });

  test('[P1] should handle undefined drift value', async ({ request }) => {
    // Given model outputs without drift
    const modelOutputs = {
      winner: 'Team A',
      confidence: 0.85,
      edge: 0.05,
      // drift is undefined
    };

    // When evaluating policy
    const response = await request.post('/api/policy/evaluate', {
      data: { modelOutputs },
    });

    // Then drift gate should pass (undefined < 0.1)
    expect(response.status()).toBe(200);
    const result = await response.json();
    expect(result.gates.driftGate).toBe('passed');
  });

  test('[P1] should handle zero confidence', async ({ request }) => {
    // Given model outputs with zero confidence
    const modelOutputs = {
      winner: 'Team A',
      confidence: 0,
      edge: 0.05,
      drift: 0.05,
    };

    // When evaluating policy
    const response = await request.post('/api/policy/evaluate', {
      data: { modelOutputs },
    });

    // Then confidence gate should fail
    expect(response.status()).toBe(200);
    const result = await response.json();
    expect(result.gates.confidenceGate).toBe('failed');
    expect(result.status).toBe('No-Bet');
  });

  test('[P1] should handle maximum confidence (1.0)', async ({ request }) => {
    // Given model outputs with maximum confidence
    const modelOutputs = {
      winner: 'Team A',
      confidence: 1.0,
      edge: 0.05,
      drift: 0.05,
    };

    // When evaluating policy
    const response = await request.post('/api/policy/evaluate', {
      data: { modelOutputs },
    });

    // Then confidence gate should pass
    expect(response.status()).toBe(200);
    const result = await response.json();
    expect(result.gates.confidenceGate).toBe('passed');
    expect(result.status).toBe('Pick');
  });

  test('[P1] should handle negative edge value', async ({ request }) => {
    // Given model outputs with negative edge
    const modelOutputs = {
      winner: 'Team A',
      confidence: 0.85,
      edge: -0.05,
      drift: 0.05,
    };

    // When evaluating policy
    const response = await request.post('/api/policy/evaluate', {
      data: { modelOutputs },
    });

    // Then edge gate should fail (negative < 0.02)
    expect(response.status()).toBe(200);
    const result = await response.json();
    expect(result.gates.edgeGate).toBe('failed');
  });

  test('[P1] should handle all gates failing', async ({ request }) => {
    // Given model outputs with all gates failing
    const modelOutputs = {
      winner: 'Team A',
      confidence: 0.3, // Fails confidence
      edge: 0.01, // Fails edge
      drift: 0.15, // Fails drift
    };

    // When evaluating policy
    const response = await request.post('/api/policy/evaluate', {
      data: { modelOutputs },
    });

    // Then all gates should fail
    expect(response.status()).toBe(200);
    const result = await response.json();
    expect(result.gates.confidenceGate).toBe('failed');
    expect(result.gates.edgeGate).toBe('failed');
    expect(result.gates.driftGate).toBe('failed');
    expect(result.status).toBe('No-Bet');
    expect(result.rationale).toContain('confidence');
  });

  test('[P1] should include evaluatedAt timestamp', async ({ request }) => {
    // Given valid model outputs
    const modelOutputs = {
      winner: 'Team A',
      confidence: 0.85,
      edge: 0.05,
      drift: 0.05,
    };

    // When evaluating policy
    const beforeRequest = Date.now();
    const response = await request.post('/api/policy/evaluate', {
      data: { modelOutputs },
    });
    const afterRequest = Date.now();

    // Then response should include valid timestamp
    expect(response.status()).toBe(200);
    const result = await response.json();
    expect(result.evaluatedAt).toBeDefined();
    const evaluatedAt = new Date(result.evaluatedAt).getTime();
    expect(evaluatedAt).toBeGreaterThanOrEqual(beforeRequest);
    expect(evaluatedAt).toBeLessThanOrEqual(afterRequest);
  });

  test('[P2] should handle missing modelOutputs', async ({ request }) => {
    // When evaluating without modelOutputs
    const response = await request.post('/api/policy/evaluate', {
      data: {},
    });

    // Then should return bad request
    expect(response.status()).toBe(400);
    const body = await response.json();
    expect(body.error).toContain('Missing modelOutputs');
  });

  test('[P2] should handle empty modelOutputs object', async ({ request }) => {
    // Given empty modelOutputs
    const modelOutputs = {};

    // When evaluating policy
    const response = await request.post('/api/policy/evaluate', {
      data: { modelOutputs },
    });

    // Then should handle gracefully (gates use undefined values)
    expect(response.status()).toBe(200);
    const result = await response.json();
    expect(result.gates).toBeDefined();
  });

  test('[P2] should handle very large drift values', async ({ request }) => {
    // Given model outputs with very large drift
    const modelOutputs = {
      winner: 'Team A',
      confidence: 0.85,
      edge: 0.05,
      drift: 999.99,
    };

    // When evaluating policy
    const response = await request.post('/api/policy/evaluate', {
      data: { modelOutputs },
    });

    // Then drift gate should fail
    expect(response.status()).toBe(200);
    const result = await response.json();
    expect(result.gates.driftGate).toBe('failed');
  });
});
